<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>COMP5421 - Computer Vision</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }

        nav {
            background-color: #2c3e50;
            color: white;
            padding: 1rem;
            position: fixed;
            width: 100%;
            top: 0;
        }

        nav a {
            color: white;
            text-decoration: none;
            margin-right: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 80px auto 40px;
            padding: 0 20px;
        }

        section {
            margin-bottom: 40px;
            background-color: #f9f9f9;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        h1, h2 {
            color: #2c3e50;
        }

        footer {
            background-color: #2c3e50;
            color: white;
            text-align: center;
            padding: 1rem;
            position: relative;
            bottom: 0;
            width: 100%;
        }
    </style>
</head>
<body>
    <nav>
        <a href="#description">Project Description</a>
        <!-- <a href="#instructor">Instructor</a> -->
        <!-- <a href="#tas">TAs</a> -->
        <a href="#Groups">Groups</a>
        <a href="#resources">Resources</a>
        <a href="#tutorials">Tutorials</a>
    </nav>

    <div class="container">
        <section id="description">
            <h1>COMP5421 - Computer Vision</h1>
            <h2>Instructor</h2>
            <ul>
                <li>Prof. Long QUAN</li>
                <li>Email: <a href="mailto:quan@cse.ust.hk">quan@cse.ust.hk</a></li>
            </ul>
            <h2>TAs</h2>
            <ul>
                <li>Dehao Hao (dhaoab@connect.ust.hk)</li>
                <li>Kuan LI (klibs@cse.ust.hk)</li>
            </ul>
            <h2>Project Description</h2>
            <h3>Visual generations of images or 3D objects</h3>
            <p>
                This year, we only have one project but two stages. It means that you have to prepare some results and a presentation around mid-term. And you will continue your project to explore more in the rest of the semester and give a final presentation.
            </p>
            <p>
                You can explore anything if it is related to generative computer vision, for example, image generation, image super resolution, image inpainting, 3D generation and more.
            </p>
            <p>
                You can work on this project in any group of 1-2 students.
            </p>
            <h2>Groups</h2>
            <p>
                You can access to this <a href="https://docs.google.com/spreadsheets/d/1M1BzAhYCdTnfkkivZ4oB7BKWI1R4YnXznmSrjZmlTIU/edit?usp=sharing">link</a> to enter your group information.
            </p>
            
        </section>
<!-- 
        <section id="instructor">
            <h2>Instructor</h2>
            <ul>
                <li>Prof. Long QUAN</li>
                <li>Email: <a href="mailto:quan@cse.ust.hk">quan@cse.ust.hk</a></li>
            </ul>
        </section> -->

        <!-- <section id="tas">
            <h2>Teaching Assistants</h2>
            <ul>
                <li>Dehao Hao (dhaoab@connect.ust.hk)</li>
                <li>Kuan LI (klibs@cse.ust.hk)</li>
            </ul>
        </section> -->

        <section id="resources">
            <h2>Course Resources</h2>
            <h3>Deep Generative Models</h3>
            <p>
                Deep generative models are a class of methods that train deep neural networks to model the distribution of data samples. Recently, several generative models have achieved impressive results in image synthesis, image translation, 3D generation, and more. These include <strong>VAEs, GANs, autoregressive models, flow-based models, and diffusion models.</strong>
            </p>
            <p>
                Here, we recommand you to use <strong>diffusion models or flow-based models</strong> in your project, due to their state-of-the-art performance.
            </p>
            <p>
                There are some useful tutorials for deep generative models:
                <ul>
                    <li><a href="https://deepgenerativemodels.github.io/">Stanford CS236: Deep generative models</a></li>
                    <li>Lil'Log: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Diffusion Models</a>, <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">VAEs</a>, <a href="https://lilianweng.github.io/posts/2017-08-20-gan/">GANs</a>, <a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">flow-based models</a></li>
                    <li><a href="https://yang-song.net/blog/2021/score/">Score-based generative modeling</a> by Yang Song</li>
                </ul>
            </p>
            <h3>
                Dataset
            </h3>
            <p>
                We list some commonly used datasets for generative computer vision.
                <ul>
                    <li><strong>MNIST:</strong> A tiny dataset consisting of 70000 handwritten digits which are grayscale and 28x28 pixels in size.</li>
                    <li><strong>CIFAR-10:</strong> The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes.</em></strong></li>
                    <li><strong>CeleA:</strong> CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. </li>
                    <li><strong>Tiny-ImageNet: </strong>Tiny ImageNet is a subset of ImageNet dataset, which contains 100,000 images of 200 classes (500 for each class) downsized to 64x64 colored images.</li>
                    <li><strong>ShapeNetCore:</strong> ShapeNetCore covers 55 common object categories with about 51300 unique 3D models. </li>
                </ul>
                <p>
                    You can get use of first three image datasets with <strong><em>torchvision.datasets.dataset_name</em></strong> easily. 
                </p>
                <p>
                    For Tiny-ImageNet, you can download it from <a href="https://huggingface.co/datasets/zh-plus/tiny-imagenet">huggingface</a>.
                </p>
                <p>
                    For ShapeNetCore, you can log in to the <a href="https://shapenet.org/" target="_blank">ShapeNet</a> website to apply for the dataset, or download preprocessed dataset by previous works directly(for example: <a href="https://github.com/1zb/3DShape2VecSet" target="_blank">3DShape2VecSet</a>).
                </p>
            </p>
          
        </section>

        <section id="tutorials">
            <h2>Tutorials & Links</h2>
            <p>
                We provide two simple implementations for diffusion models and flow matching models to help you get started quickly if you don't have prior knowledge.
            </p>
            <ul>
                <li>
                    Here is a <a href="https://github.com/dhoho2002/GenVision/blob/main/diffusion.ipynb">simple implementation</a> for diffusion model with <a href="https://huggingface.co/docs/diffusers/en/index">diffusers</a>.
                </li>
                <li>
                    For flow-based model, we recommand you to refer to torchcfm <a href="https://github.com/atong01/conditional-flow-matching/blob/main/examples/2D_tutorials/Flow_matching_tutorial.ipynb">2D toy example</a> and <a href="https://github.com/atong01/conditional-flow-matching/blob/main/examples/images/mnist_example.ipynb"> MNIST examples</a>.
                </li>
            </ul>
        </section>
    </div>

    <footer>
        <p>&copy; 2025 COMP5421 - Computer Vision Course. All rights reserved.</p>
    </footer>
</body>
</html>
